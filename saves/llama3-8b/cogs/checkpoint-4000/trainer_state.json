{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7948533247224223,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0019871333118060557,
      "grad_norm": 5.92092752456665,
      "learning_rate": 2.99997076666703e-05,
      "loss": 5.6654,
      "step": 10
    },
    {
      "epoch": 0.003974266623612111,
      "grad_norm": 4.2136759757995605,
      "learning_rate": 2.9998830678075706e-05,
      "loss": 4.2674,
      "step": 20
    },
    {
      "epoch": 0.0059613999354181674,
      "grad_norm": 3.511962652206421,
      "learning_rate": 2.9997369068399283e-05,
      "loss": 3.0076,
      "step": 30
    },
    {
      "epoch": 0.007948533247224223,
      "grad_norm": 3.612217426300049,
      "learning_rate": 2.9995322894611327e-05,
      "loss": 2.1897,
      "step": 40
    },
    {
      "epoch": 0.00993566655903028,
      "grad_norm": 2.599956750869751,
      "learning_rate": 2.9992692236467143e-05,
      "loss": 1.6665,
      "step": 50
    },
    {
      "epoch": 0.011922799870836335,
      "grad_norm": 2.2277534008026123,
      "learning_rate": 2.998947719650394e-05,
      "loss": 1.4466,
      "step": 60
    },
    {
      "epoch": 0.01390993318264239,
      "grad_norm": 2.3141391277313232,
      "learning_rate": 2.998567790003683e-05,
      "loss": 1.2556,
      "step": 70
    },
    {
      "epoch": 0.015897066494448445,
      "grad_norm": 2.342633008956909,
      "learning_rate": 2.9981294495153947e-05,
      "loss": 1.1132,
      "step": 80
    },
    {
      "epoch": 0.0178841998062545,
      "grad_norm": 2.4862706661224365,
      "learning_rate": 2.9976327152710662e-05,
      "loss": 1.0318,
      "step": 90
    },
    {
      "epoch": 0.01987133311806056,
      "grad_norm": 3.0390284061431885,
      "learning_rate": 2.997077606632295e-05,
      "loss": 0.9214,
      "step": 100
    },
    {
      "epoch": 0.021858466429866615,
      "grad_norm": 2.664877414703369,
      "learning_rate": 2.996464145235982e-05,
      "loss": 0.7819,
      "step": 110
    },
    {
      "epoch": 0.02384559974167267,
      "grad_norm": 2.610596179962158,
      "learning_rate": 2.9957923549934884e-05,
      "loss": 0.6811,
      "step": 120
    },
    {
      "epoch": 0.025832733053478725,
      "grad_norm": 2.0170514583587646,
      "learning_rate": 2.995062262089705e-05,
      "loss": 0.6278,
      "step": 130
    },
    {
      "epoch": 0.02781986636528478,
      "grad_norm": 2.0646398067474365,
      "learning_rate": 2.99427389498203e-05,
      "loss": 0.614,
      "step": 140
    },
    {
      "epoch": 0.029806999677090835,
      "grad_norm": 1.8415906429290771,
      "learning_rate": 2.993427284399262e-05,
      "loss": 0.5582,
      "step": 150
    },
    {
      "epoch": 0.03179413298889689,
      "grad_norm": 1.6610714197158813,
      "learning_rate": 2.9925224633403982e-05,
      "loss": 0.5426,
      "step": 160
    },
    {
      "epoch": 0.033781266300702946,
      "grad_norm": 1.5875061750411987,
      "learning_rate": 2.9915594670733538e-05,
      "loss": 0.5445,
      "step": 170
    },
    {
      "epoch": 0.035768399612509,
      "grad_norm": 1.490470290184021,
      "learning_rate": 2.990538333133582e-05,
      "loss": 0.528,
      "step": 180
    },
    {
      "epoch": 0.03775553292431506,
      "grad_norm": 1.9577637910842896,
      "learning_rate": 2.989459101322614e-05,
      "loss": 0.5238,
      "step": 190
    },
    {
      "epoch": 0.03974266623612112,
      "grad_norm": 1.4940285682678223,
      "learning_rate": 2.9883218137065077e-05,
      "loss": 0.5211,
      "step": 200
    },
    {
      "epoch": 0.041729799547927174,
      "grad_norm": 1.4702329635620117,
      "learning_rate": 2.9871265146142058e-05,
      "loss": 0.5055,
      "step": 210
    },
    {
      "epoch": 0.04371693285973323,
      "grad_norm": 1.1786497831344604,
      "learning_rate": 2.9858732506358102e-05,
      "loss": 0.519,
      "step": 220
    },
    {
      "epoch": 0.045704066171539284,
      "grad_norm": 1.0310534238815308,
      "learning_rate": 2.9845620706207657e-05,
      "loss": 0.4992,
      "step": 230
    },
    {
      "epoch": 0.04769119948334534,
      "grad_norm": 1.5403022766113281,
      "learning_rate": 2.9831930256759542e-05,
      "loss": 0.494,
      "step": 240
    },
    {
      "epoch": 0.049678332795151395,
      "grad_norm": 1.255388617515564,
      "learning_rate": 2.981766169163705e-05,
      "loss": 0.5027,
      "step": 250
    },
    {
      "epoch": 0.05166546610695745,
      "grad_norm": 1.2527488470077515,
      "learning_rate": 2.980281556699714e-05,
      "loss": 0.5086,
      "step": 260
    },
    {
      "epoch": 0.053652599418763505,
      "grad_norm": 1.3441498279571533,
      "learning_rate": 2.978739246150874e-05,
      "loss": 0.5082,
      "step": 270
    },
    {
      "epoch": 0.05563973273056956,
      "grad_norm": 1.122584342956543,
      "learning_rate": 2.977139297633023e-05,
      "loss": 0.489,
      "step": 280
    },
    {
      "epoch": 0.057626866042375616,
      "grad_norm": 1.3364392518997192,
      "learning_rate": 2.975481773508598e-05,
      "loss": 0.4906,
      "step": 290
    },
    {
      "epoch": 0.05961399935418167,
      "grad_norm": 1.6821298599243164,
      "learning_rate": 2.9737667383842048e-05,
      "loss": 0.4803,
      "step": 300
    },
    {
      "epoch": 0.061601132665987726,
      "grad_norm": 1.2908855676651,
      "learning_rate": 2.9719942591081005e-05,
      "loss": 0.4884,
      "step": 310
    },
    {
      "epoch": 0.06358826597779378,
      "grad_norm": 1.2065061330795288,
      "learning_rate": 2.9701644047675877e-05,
      "loss": 0.4834,
      "step": 320
    },
    {
      "epoch": 0.06557539928959984,
      "grad_norm": 1.232344150543213,
      "learning_rate": 2.968277246686322e-05,
      "loss": 0.4865,
      "step": 330
    },
    {
      "epoch": 0.06756253260140589,
      "grad_norm": 1.1818547248840332,
      "learning_rate": 2.9663328584215294e-05,
      "loss": 0.4802,
      "step": 340
    },
    {
      "epoch": 0.06954966591321195,
      "grad_norm": 1.3007862567901611,
      "learning_rate": 2.9643313157611436e-05,
      "loss": 0.4891,
      "step": 350
    },
    {
      "epoch": 0.071536799225018,
      "grad_norm": 1.2889410257339478,
      "learning_rate": 2.962272696720849e-05,
      "loss": 0.4831,
      "step": 360
    },
    {
      "epoch": 0.07352393253682406,
      "grad_norm": 1.1546827554702759,
      "learning_rate": 2.9601570815410394e-05,
      "loss": 0.4956,
      "step": 370
    },
    {
      "epoch": 0.07551106584863013,
      "grad_norm": 1.104743480682373,
      "learning_rate": 2.9579845526836923e-05,
      "loss": 0.4844,
      "step": 380
    },
    {
      "epoch": 0.07749819916043618,
      "grad_norm": 1.397678017616272,
      "learning_rate": 2.9557551948291534e-05,
      "loss": 0.4815,
      "step": 390
    },
    {
      "epoch": 0.07948533247224224,
      "grad_norm": 1.4571044445037842,
      "learning_rate": 2.9534690948728373e-05,
      "loss": 0.4864,
      "step": 400
    },
    {
      "epoch": 0.08147246578404829,
      "grad_norm": 1.203877568244934,
      "learning_rate": 2.9511263419218386e-05,
      "loss": 0.4825,
      "step": 410
    },
    {
      "epoch": 0.08345959909585435,
      "grad_norm": 1.5624138116836548,
      "learning_rate": 2.94872702729146e-05,
      "loss": 0.4786,
      "step": 420
    },
    {
      "epoch": 0.0854467324076604,
      "grad_norm": 0.9051435589790344,
      "learning_rate": 2.9462712445016536e-05,
      "loss": 0.482,
      "step": 430
    },
    {
      "epoch": 0.08743386571946646,
      "grad_norm": 1.126031756401062,
      "learning_rate": 2.9437590892733726e-05,
      "loss": 0.4693,
      "step": 440
    },
    {
      "epoch": 0.08942099903127251,
      "grad_norm": 1.1019513607025146,
      "learning_rate": 2.9411906595248448e-05,
      "loss": 0.4736,
      "step": 450
    },
    {
      "epoch": 0.09140813234307857,
      "grad_norm": 1.0924700498580933,
      "learning_rate": 2.9385660553677532e-05,
      "loss": 0.4769,
      "step": 460
    },
    {
      "epoch": 0.09339526565488462,
      "grad_norm": 1.5651737451553345,
      "learning_rate": 2.935885379103334e-05,
      "loss": 0.4769,
      "step": 470
    },
    {
      "epoch": 0.09538239896669068,
      "grad_norm": 0.9393084645271301,
      "learning_rate": 2.933148735218389e-05,
      "loss": 0.4756,
      "step": 480
    },
    {
      "epoch": 0.09736953227849673,
      "grad_norm": 0.9699288010597229,
      "learning_rate": 2.930356230381215e-05,
      "loss": 0.4701,
      "step": 490
    },
    {
      "epoch": 0.09935666559030279,
      "grad_norm": 0.9283477067947388,
      "learning_rate": 2.927507973437443e-05,
      "loss": 0.4714,
      "step": 500
    },
    {
      "epoch": 0.10134379890210884,
      "grad_norm": 1.0415772199630737,
      "learning_rate": 2.924604075405798e-05,
      "loss": 0.4721,
      "step": 510
    },
    {
      "epoch": 0.1033309322139149,
      "grad_norm": 0.9317978024482727,
      "learning_rate": 2.9216446494737707e-05,
      "loss": 0.4719,
      "step": 520
    },
    {
      "epoch": 0.10531806552572096,
      "grad_norm": 1.0949742794036865,
      "learning_rate": 2.9186298109932063e-05,
      "loss": 0.4742,
      "step": 530
    },
    {
      "epoch": 0.10730519883752701,
      "grad_norm": 0.9628154039382935,
      "learning_rate": 2.915559677475807e-05,
      "loss": 0.467,
      "step": 540
    },
    {
      "epoch": 0.10929233214933307,
      "grad_norm": 1.9474523067474365,
      "learning_rate": 2.912434368588554e-05,
      "loss": 0.4632,
      "step": 550
    },
    {
      "epoch": 0.11127946546113912,
      "grad_norm": 1.1614058017730713,
      "learning_rate": 2.9092540061490414e-05,
      "loss": 0.47,
      "step": 560
    },
    {
      "epoch": 0.11326659877294518,
      "grad_norm": 1.0519062280654907,
      "learning_rate": 2.9060187141207268e-05,
      "loss": 0.465,
      "step": 570
    },
    {
      "epoch": 0.11525373208475123,
      "grad_norm": 0.9110799431800842,
      "learning_rate": 2.9027286186081033e-05,
      "loss": 0.4785,
      "step": 580
    },
    {
      "epoch": 0.11724086539655729,
      "grad_norm": 1.018890380859375,
      "learning_rate": 2.899383847851781e-05,
      "loss": 0.4691,
      "step": 590
    },
    {
      "epoch": 0.11922799870836334,
      "grad_norm": 0.9602355360984802,
      "learning_rate": 2.895984532223489e-05,
      "loss": 0.4704,
      "step": 600
    },
    {
      "epoch": 0.1212151320201694,
      "grad_norm": 1.0354691743850708,
      "learning_rate": 2.8925308042209953e-05,
      "loss": 0.4686,
      "step": 610
    },
    {
      "epoch": 0.12320226533197545,
      "grad_norm": 0.9470781087875366,
      "learning_rate": 2.8890227984629405e-05,
      "loss": 0.459,
      "step": 620
    },
    {
      "epoch": 0.1251893986437815,
      "grad_norm": 0.9859604239463806,
      "learning_rate": 2.8854606516835923e-05,
      "loss": 0.4634,
      "step": 630
    },
    {
      "epoch": 0.12717653195558756,
      "grad_norm": 0.9412305355072021,
      "learning_rate": 2.8818445027275143e-05,
      "loss": 0.4609,
      "step": 640
    },
    {
      "epoch": 0.12916366526739362,
      "grad_norm": 1.0385119915008545,
      "learning_rate": 2.8781744925441547e-05,
      "loss": 0.4619,
      "step": 650
    },
    {
      "epoch": 0.13115079857919967,
      "grad_norm": 0.9231472015380859,
      "learning_rate": 2.8744507641823537e-05,
      "loss": 0.4645,
      "step": 660
    },
    {
      "epoch": 0.13313793189100573,
      "grad_norm": 1.0851837396621704,
      "learning_rate": 2.870673462784766e-05,
      "loss": 0.4688,
      "step": 670
    },
    {
      "epoch": 0.13512506520281178,
      "grad_norm": 0.9515172839164734,
      "learning_rate": 2.8668427355822036e-05,
      "loss": 0.4658,
      "step": 680
    },
    {
      "epoch": 0.13711219851461784,
      "grad_norm": 0.7995389103889465,
      "learning_rate": 2.862958731887899e-05,
      "loss": 0.4697,
      "step": 690
    },
    {
      "epoch": 0.1390993318264239,
      "grad_norm": 1.0087746381759644,
      "learning_rate": 2.859021603091683e-05,
      "loss": 0.4617,
      "step": 700
    },
    {
      "epoch": 0.14108646513822995,
      "grad_norm": 1.0131690502166748,
      "learning_rate": 2.8550315026540854e-05,
      "loss": 0.4575,
      "step": 710
    },
    {
      "epoch": 0.143073598450036,
      "grad_norm": 1.0719910860061646,
      "learning_rate": 2.8509885861003518e-05,
      "loss": 0.4696,
      "step": 720
    },
    {
      "epoch": 0.14506073176184206,
      "grad_norm": 0.8925458192825317,
      "learning_rate": 2.8468930110143834e-05,
      "loss": 0.4658,
      "step": 730
    },
    {
      "epoch": 0.14704786507364812,
      "grad_norm": 1.2844123840332031,
      "learning_rate": 2.8427449370325938e-05,
      "loss": 0.4533,
      "step": 740
    },
    {
      "epoch": 0.14903499838545417,
      "grad_norm": 1.4318522214889526,
      "learning_rate": 2.8385445258376868e-05,
      "loss": 0.4602,
      "step": 750
    },
    {
      "epoch": 0.15102213169726025,
      "grad_norm": 0.9566059112548828,
      "learning_rate": 2.8342919411523545e-05,
      "loss": 0.4561,
      "step": 760
    },
    {
      "epoch": 0.1530092650090663,
      "grad_norm": 1.0394426584243774,
      "learning_rate": 2.8299873487328964e-05,
      "loss": 0.4637,
      "step": 770
    },
    {
      "epoch": 0.15499639832087236,
      "grad_norm": 1.11858069896698,
      "learning_rate": 2.825630916362756e-05,
      "loss": 0.4628,
      "step": 780
    },
    {
      "epoch": 0.15698353163267842,
      "grad_norm": 0.9832200407981873,
      "learning_rate": 2.821222813845985e-05,
      "loss": 0.4646,
      "step": 790
    },
    {
      "epoch": 0.15897066494448447,
      "grad_norm": 1.4214552640914917,
      "learning_rate": 2.8167632130006203e-05,
      "loss": 0.4784,
      "step": 800
    },
    {
      "epoch": 0.16095779825629053,
      "grad_norm": 0.7801896333694458,
      "learning_rate": 2.812252287651992e-05,
      "loss": 0.4566,
      "step": 810
    },
    {
      "epoch": 0.16294493156809658,
      "grad_norm": 0.8572341203689575,
      "learning_rate": 2.8076902136259426e-05,
      "loss": 0.4592,
      "step": 820
    },
    {
      "epoch": 0.16493206487990264,
      "grad_norm": 0.9282078742980957,
      "learning_rate": 2.8030771687419787e-05,
      "loss": 0.4658,
      "step": 830
    },
    {
      "epoch": 0.1669191981917087,
      "grad_norm": 0.9376913905143738,
      "learning_rate": 2.7984133328063354e-05,
      "loss": 0.46,
      "step": 840
    },
    {
      "epoch": 0.16890633150351475,
      "grad_norm": 0.8676624894142151,
      "learning_rate": 2.793698887604971e-05,
      "loss": 0.4674,
      "step": 850
    },
    {
      "epoch": 0.1708934648153208,
      "grad_norm": 1.2927274703979492,
      "learning_rate": 2.788934016896481e-05,
      "loss": 0.4589,
      "step": 860
    },
    {
      "epoch": 0.17288059812712686,
      "grad_norm": 1.4049725532531738,
      "learning_rate": 2.784118906404935e-05,
      "loss": 0.4548,
      "step": 870
    },
    {
      "epoch": 0.17486773143893292,
      "grad_norm": 0.9724503755569458,
      "learning_rate": 2.779253743812637e-05,
      "loss": 0.4603,
      "step": 880
    },
    {
      "epoch": 0.17685486475073897,
      "grad_norm": 1.7718671560287476,
      "learning_rate": 2.774338718752811e-05,
      "loss": 0.4552,
      "step": 890
    },
    {
      "epoch": 0.17884199806254503,
      "grad_norm": 0.8345943689346313,
      "learning_rate": 2.7693740228022098e-05,
      "loss": 0.4547,
      "step": 900
    },
    {
      "epoch": 0.18082913137435108,
      "grad_norm": 0.8909539580345154,
      "learning_rate": 2.7643598494736456e-05,
      "loss": 0.4651,
      "step": 910
    },
    {
      "epoch": 0.18281626468615714,
      "grad_norm": 0.9543066024780273,
      "learning_rate": 2.7592963942084498e-05,
      "loss": 0.4716,
      "step": 920
    },
    {
      "epoch": 0.1848033979979632,
      "grad_norm": 0.906904399394989,
      "learning_rate": 2.7541838543688546e-05,
      "loss": 0.4538,
      "step": 930
    },
    {
      "epoch": 0.18679053130976925,
      "grad_norm": 0.7959210276603699,
      "learning_rate": 2.7490224292302995e-05,
      "loss": 0.4682,
      "step": 940
    },
    {
      "epoch": 0.1887776646215753,
      "grad_norm": 0.8484536409378052,
      "learning_rate": 2.7438123199736627e-05,
      "loss": 0.4646,
      "step": 950
    },
    {
      "epoch": 0.19076479793338136,
      "grad_norm": 0.8852921724319458,
      "learning_rate": 2.738553729677424e-05,
      "loss": 0.4555,
      "step": 960
    },
    {
      "epoch": 0.1927519312451874,
      "grad_norm": 0.8975186944007874,
      "learning_rate": 2.733246863309744e-05,
      "loss": 0.4615,
      "step": 970
    },
    {
      "epoch": 0.19473906455699347,
      "grad_norm": 0.8983160853385925,
      "learning_rate": 2.7278919277204786e-05,
      "loss": 0.456,
      "step": 980
    },
    {
      "epoch": 0.19672619786879952,
      "grad_norm": 0.9357101321220398,
      "learning_rate": 2.7224891316331142e-05,
      "loss": 0.4566,
      "step": 990
    },
    {
      "epoch": 0.19871333118060558,
      "grad_norm": 0.883109986782074,
      "learning_rate": 2.7170386856366334e-05,
      "loss": 0.4577,
      "step": 1000
    },
    {
      "epoch": 0.20070046449241163,
      "grad_norm": 0.9193719625473022,
      "learning_rate": 2.7115408021773066e-05,
      "loss": 0.4552,
      "step": 1010
    },
    {
      "epoch": 0.2026875978042177,
      "grad_norm": 0.8173915147781372,
      "learning_rate": 2.705995695550411e-05,
      "loss": 0.4566,
      "step": 1020
    },
    {
      "epoch": 0.20467473111602374,
      "grad_norm": 0.9621852040290833,
      "learning_rate": 2.7004035818918775e-05,
      "loss": 0.4536,
      "step": 1030
    },
    {
      "epoch": 0.2066618644278298,
      "grad_norm": 0.8913636803627014,
      "learning_rate": 2.694764679169867e-05,
      "loss": 0.4597,
      "step": 1040
    },
    {
      "epoch": 0.20864899773963586,
      "grad_norm": 0.9341599345207214,
      "learning_rate": 2.6890792071762744e-05,
      "loss": 0.4583,
      "step": 1050
    },
    {
      "epoch": 0.2106361310514419,
      "grad_norm": 0.9077039957046509,
      "learning_rate": 2.6833473875181596e-05,
      "loss": 0.4467,
      "step": 1060
    },
    {
      "epoch": 0.21262326436324797,
      "grad_norm": 0.8550469279289246,
      "learning_rate": 2.677569443609114e-05,
      "loss": 0.4546,
      "step": 1070
    },
    {
      "epoch": 0.21461039767505402,
      "grad_norm": 0.8296453356742859,
      "learning_rate": 2.671745600660548e-05,
      "loss": 0.4509,
      "step": 1080
    },
    {
      "epoch": 0.21659753098686008,
      "grad_norm": 0.8400294780731201,
      "learning_rate": 2.6658760856729157e-05,
      "loss": 0.4542,
      "step": 1090
    },
    {
      "epoch": 0.21858466429866613,
      "grad_norm": 0.8238516449928284,
      "learning_rate": 2.6599611274268637e-05,
      "loss": 0.454,
      "step": 1100
    },
    {
      "epoch": 0.2205717976104722,
      "grad_norm": 0.8118131160736084,
      "learning_rate": 2.6540009564743186e-05,
      "loss": 0.4575,
      "step": 1110
    },
    {
      "epoch": 0.22255893092227824,
      "grad_norm": 0.8206901550292969,
      "learning_rate": 2.6479958051294952e-05,
      "loss": 0.4592,
      "step": 1120
    },
    {
      "epoch": 0.2245460642340843,
      "grad_norm": 0.8222482800483704,
      "learning_rate": 2.6419459074598464e-05,
      "loss": 0.4594,
      "step": 1130
    },
    {
      "epoch": 0.22653319754589035,
      "grad_norm": 0.9001193046569824,
      "learning_rate": 2.6358514992769357e-05,
      "loss": 0.4462,
      "step": 1140
    },
    {
      "epoch": 0.2285203308576964,
      "grad_norm": 0.7622286677360535,
      "learning_rate": 2.6297128181272477e-05,
      "loss": 0.4512,
      "step": 1150
    },
    {
      "epoch": 0.23050746416950246,
      "grad_norm": 0.7960866689682007,
      "learning_rate": 2.62353010328293e-05,
      "loss": 0.4569,
      "step": 1160
    },
    {
      "epoch": 0.23249459748130852,
      "grad_norm": 0.8378702998161316,
      "learning_rate": 2.6173035957324637e-05,
      "loss": 0.4535,
      "step": 1170
    },
    {
      "epoch": 0.23448173079311457,
      "grad_norm": 1.0041275024414062,
      "learning_rate": 2.611033538171274e-05,
      "loss": 0.4601,
      "step": 1180
    },
    {
      "epoch": 0.23646886410492063,
      "grad_norm": 0.7622418403625488,
      "learning_rate": 2.6047201749922683e-05,
      "loss": 0.4579,
      "step": 1190
    },
    {
      "epoch": 0.23845599741672668,
      "grad_norm": 0.9227501749992371,
      "learning_rate": 2.5983637522763105e-05,
      "loss": 0.4535,
      "step": 1200
    },
    {
      "epoch": 0.24044313072853274,
      "grad_norm": 0.8607071042060852,
      "learning_rate": 2.5919645177826295e-05,
      "loss": 0.4688,
      "step": 1210
    },
    {
      "epoch": 0.2424302640403388,
      "grad_norm": 0.7852434515953064,
      "learning_rate": 2.585522720939162e-05,
      "loss": 0.4565,
      "step": 1220
    },
    {
      "epoch": 0.24441739735214485,
      "grad_norm": 0.7894493937492371,
      "learning_rate": 2.579038612832831e-05,
      "loss": 0.452,
      "step": 1230
    },
    {
      "epoch": 0.2464045306639509,
      "grad_norm": 0.7203909158706665,
      "learning_rate": 2.572512446199758e-05,
      "loss": 0.4487,
      "step": 1240
    },
    {
      "epoch": 0.24839166397575696,
      "grad_norm": 0.8323131799697876,
      "learning_rate": 2.5659444754154132e-05,
      "loss": 0.4667,
      "step": 1250
    },
    {
      "epoch": 0.250378797287563,
      "grad_norm": 0.815660297870636,
      "learning_rate": 2.559334956484699e-05,
      "loss": 0.4576,
      "step": 1260
    },
    {
      "epoch": 0.25236593059936907,
      "grad_norm": 0.8667727112770081,
      "learning_rate": 2.552684147031971e-05,
      "loss": 0.4515,
      "step": 1270
    },
    {
      "epoch": 0.2543530639111751,
      "grad_norm": 0.8607484698295593,
      "learning_rate": 2.5459923062910004e-05,
      "loss": 0.4604,
      "step": 1280
    },
    {
      "epoch": 0.2563401972229812,
      "grad_norm": 0.8368757367134094,
      "learning_rate": 2.5392596950948647e-05,
      "loss": 0.4578,
      "step": 1290
    },
    {
      "epoch": 0.25832733053478724,
      "grad_norm": 0.8993106484413147,
      "learning_rate": 2.532486575865784e-05,
      "loss": 0.455,
      "step": 1300
    },
    {
      "epoch": 0.2603144638465933,
      "grad_norm": 0.746140718460083,
      "learning_rate": 2.5256732126048907e-05,
      "loss": 0.4592,
      "step": 1310
    },
    {
      "epoch": 0.26230159715839935,
      "grad_norm": 0.7810063362121582,
      "learning_rate": 2.5188198708819408e-05,
      "loss": 0.4536,
      "step": 1320
    },
    {
      "epoch": 0.2642887304702054,
      "grad_norm": 0.8129777908325195,
      "learning_rate": 2.5119268178249618e-05,
      "loss": 0.4588,
      "step": 1330
    },
    {
      "epoch": 0.26627586378201146,
      "grad_norm": 0.7747403979301453,
      "learning_rate": 2.5049943221098405e-05,
      "loss": 0.4537,
      "step": 1340
    },
    {
      "epoch": 0.2682629970938175,
      "grad_norm": 0.8697390556335449,
      "learning_rate": 2.498022653949851e-05,
      "loss": 0.4491,
      "step": 1350
    },
    {
      "epoch": 0.27025013040562357,
      "grad_norm": 0.792535126209259,
      "learning_rate": 2.491012085085122e-05,
      "loss": 0.4549,
      "step": 1360
    },
    {
      "epoch": 0.2722372637174296,
      "grad_norm": 0.9221652150154114,
      "learning_rate": 2.483962888772046e-05,
      "loss": 0.4565,
      "step": 1370
    },
    {
      "epoch": 0.2742243970292357,
      "grad_norm": 0.7828012108802795,
      "learning_rate": 2.4768753397726266e-05,
      "loss": 0.4559,
      "step": 1380
    },
    {
      "epoch": 0.27621153034104173,
      "grad_norm": 0.7941563725471497,
      "learning_rate": 2.46974971434377e-05,
      "loss": 0.4508,
      "step": 1390
    },
    {
      "epoch": 0.2781986636528478,
      "grad_norm": 0.8169874548912048,
      "learning_rate": 2.462586290226518e-05,
      "loss": 0.4548,
      "step": 1400
    },
    {
      "epoch": 0.28018579696465384,
      "grad_norm": 0.7765164375305176,
      "learning_rate": 2.4553853466352197e-05,
      "loss": 0.447,
      "step": 1410
    },
    {
      "epoch": 0.2821729302764599,
      "grad_norm": 0.7228377461433411,
      "learning_rate": 2.448147164246651e-05,
      "loss": 0.4568,
      "step": 1420
    },
    {
      "epoch": 0.28416006358826595,
      "grad_norm": 0.8615782260894775,
      "learning_rate": 2.4408720251890733e-05,
      "loss": 0.4557,
      "step": 1430
    },
    {
      "epoch": 0.286147196900072,
      "grad_norm": 0.8100447654724121,
      "learning_rate": 2.4335602130312363e-05,
      "loss": 0.4532,
      "step": 1440
    },
    {
      "epoch": 0.28813433021187806,
      "grad_norm": 0.7801563739776611,
      "learning_rate": 2.4262120127713257e-05,
      "loss": 0.4534,
      "step": 1450
    },
    {
      "epoch": 0.2901214635236841,
      "grad_norm": 0.9016355276107788,
      "learning_rate": 2.4188277108258546e-05,
      "loss": 0.4547,
      "step": 1460
    },
    {
      "epoch": 0.2921085968354902,
      "grad_norm": 0.7537220120429993,
      "learning_rate": 2.4114075950185008e-05,
      "loss": 0.4596,
      "step": 1470
    },
    {
      "epoch": 0.29409573014729623,
      "grad_norm": 1.1073130369186401,
      "learning_rate": 2.4039519545688848e-05,
      "loss": 0.4552,
      "step": 1480
    },
    {
      "epoch": 0.2960828634591023,
      "grad_norm": 0.8423623442649841,
      "learning_rate": 2.3964610800812994e-05,
      "loss": 0.4608,
      "step": 1490
    },
    {
      "epoch": 0.29806999677090834,
      "grad_norm": 0.7566034197807312,
      "learning_rate": 2.3889352635333827e-05,
      "loss": 0.4496,
      "step": 1500
    },
    {
      "epoch": 0.30005713008271445,
      "grad_norm": 0.8332352638244629,
      "learning_rate": 2.381374798264736e-05,
      "loss": 0.4458,
      "step": 1510
    },
    {
      "epoch": 0.3020442633945205,
      "grad_norm": 0.727422297000885,
      "learning_rate": 2.3737799789654907e-05,
      "loss": 0.4526,
      "step": 1520
    },
    {
      "epoch": 0.30403139670632656,
      "grad_norm": 0.8298748731613159,
      "learning_rate": 2.3661511016648223e-05,
      "loss": 0.4668,
      "step": 1530
    },
    {
      "epoch": 0.3060185300181326,
      "grad_norm": 0.8949200510978699,
      "learning_rate": 2.3584884637194105e-05,
      "loss": 0.452,
      "step": 1540
    },
    {
      "epoch": 0.3080056633299387,
      "grad_norm": 0.709365963935852,
      "learning_rate": 2.3507923638018513e-05,
      "loss": 0.4457,
      "step": 1550
    },
    {
      "epoch": 0.30999279664174473,
      "grad_norm": 0.7112751007080078,
      "learning_rate": 2.343063101889013e-05,
      "loss": 0.4563,
      "step": 1560
    },
    {
      "epoch": 0.3119799299535508,
      "grad_norm": 0.7988191843032837,
      "learning_rate": 2.335300979250345e-05,
      "loss": 0.4564,
      "step": 1570
    },
    {
      "epoch": 0.31396706326535684,
      "grad_norm": 0.7586749196052551,
      "learning_rate": 2.3275062984361348e-05,
      "loss": 0.4471,
      "step": 1580
    },
    {
      "epoch": 0.3159541965771629,
      "grad_norm": 0.8084196448326111,
      "learning_rate": 2.3196793632657157e-05,
      "loss": 0.4535,
      "step": 1590
    },
    {
      "epoch": 0.31794132988896895,
      "grad_norm": 0.8677108287811279,
      "learning_rate": 2.311820478815623e-05,
      "loss": 0.4583,
      "step": 1600
    },
    {
      "epoch": 0.319928463200775,
      "grad_norm": 0.8106454610824585,
      "learning_rate": 2.3039299514077047e-05,
      "loss": 0.4565,
      "step": 1610
    },
    {
      "epoch": 0.32191559651258106,
      "grad_norm": 0.8341737389564514,
      "learning_rate": 2.2960080885971813e-05,
      "loss": 0.4488,
      "step": 1620
    },
    {
      "epoch": 0.3239027298243871,
      "grad_norm": 0.7227737903594971,
      "learning_rate": 2.2880551991606576e-05,
      "loss": 0.4493,
      "step": 1630
    },
    {
      "epoch": 0.32588986313619317,
      "grad_norm": 0.7682821750640869,
      "learning_rate": 2.280071593084085e-05,
      "loss": 0.4491,
      "step": 1640
    },
    {
      "epoch": 0.3278769964479992,
      "grad_norm": 0.8523364067077637,
      "learning_rate": 2.272057581550685e-05,
      "loss": 0.4455,
      "step": 1650
    },
    {
      "epoch": 0.3298641297598053,
      "grad_norm": 0.9104264974594116,
      "learning_rate": 2.2640134769288143e-05,
      "loss": 0.4566,
      "step": 1660
    },
    {
      "epoch": 0.33185126307161134,
      "grad_norm": 0.7389773726463318,
      "learning_rate": 2.2559395927597906e-05,
      "loss": 0.4493,
      "step": 1670
    },
    {
      "epoch": 0.3338383963834174,
      "grad_norm": 0.7532851696014404,
      "learning_rate": 2.2478362437456726e-05,
      "loss": 0.4534,
      "step": 1680
    },
    {
      "epoch": 0.33582552969522345,
      "grad_norm": 0.7076528072357178,
      "learning_rate": 2.2397037457369945e-05,
      "loss": 0.4566,
      "step": 1690
    },
    {
      "epoch": 0.3378126630070295,
      "grad_norm": 0.8090505003929138,
      "learning_rate": 2.231542415720452e-05,
      "loss": 0.455,
      "step": 1700
    },
    {
      "epoch": 0.33979979631883556,
      "grad_norm": 0.9645341038703918,
      "learning_rate": 2.2233525718065488e-05,
      "loss": 0.4514,
      "step": 1710
    },
    {
      "epoch": 0.3417869296306416,
      "grad_norm": 0.767427384853363,
      "learning_rate": 2.2151345332171975e-05,
      "loss": 0.4552,
      "step": 1720
    },
    {
      "epoch": 0.34377406294244767,
      "grad_norm": 0.8811359405517578,
      "learning_rate": 2.2068886202732755e-05,
      "loss": 0.4502,
      "step": 1730
    },
    {
      "epoch": 0.3457611962542537,
      "grad_norm": 0.8018588423728943,
      "learning_rate": 2.1986151543821416e-05,
      "loss": 0.4567,
      "step": 1740
    },
    {
      "epoch": 0.3477483295660598,
      "grad_norm": 0.7664651870727539,
      "learning_rate": 2.1903144580251065e-05,
      "loss": 0.449,
      "step": 1750
    },
    {
      "epoch": 0.34973546287786583,
      "grad_norm": 0.7770966291427612,
      "learning_rate": 2.181986854744864e-05,
      "loss": 0.4523,
      "step": 1760
    },
    {
      "epoch": 0.3517225961896719,
      "grad_norm": 0.8076283931732178,
      "learning_rate": 2.1736326691328805e-05,
      "loss": 0.454,
      "step": 1770
    },
    {
      "epoch": 0.35370972950147794,
      "grad_norm": 0.8766160607337952,
      "learning_rate": 2.1652522268167416e-05,
      "loss": 0.4533,
      "step": 1780
    },
    {
      "epoch": 0.355696862813284,
      "grad_norm": 0.7948440909385681,
      "learning_rate": 2.1568458544474625e-05,
      "loss": 0.4521,
      "step": 1790
    },
    {
      "epoch": 0.35768399612509005,
      "grad_norm": 0.7801744937896729,
      "learning_rate": 2.148413879686753e-05,
      "loss": 0.4531,
      "step": 1800
    },
    {
      "epoch": 0.3596711294368961,
      "grad_norm": 0.8009854555130005,
      "learning_rate": 2.1399566311942466e-05,
      "loss": 0.4506,
      "step": 1810
    },
    {
      "epoch": 0.36165826274870216,
      "grad_norm": 0.735355019569397,
      "learning_rate": 2.1314744386146926e-05,
      "loss": 0.4475,
      "step": 1820
    },
    {
      "epoch": 0.3636453960605082,
      "grad_norm": 0.815009593963623,
      "learning_rate": 2.122967632565103e-05,
      "loss": 0.4513,
      "step": 1830
    },
    {
      "epoch": 0.3656325293723143,
      "grad_norm": 0.8351150751113892,
      "learning_rate": 2.1144365446218716e-05,
      "loss": 0.451,
      "step": 1840
    },
    {
      "epoch": 0.36761966268412033,
      "grad_norm": 0.7446357607841492,
      "learning_rate": 2.1058815073078425e-05,
      "loss": 0.4509,
      "step": 1850
    },
    {
      "epoch": 0.3696067959959264,
      "grad_norm": 0.8544518947601318,
      "learning_rate": 2.0973028540793556e-05,
      "loss": 0.4459,
      "step": 1860
    },
    {
      "epoch": 0.37159392930773244,
      "grad_norm": 0.7670299410820007,
      "learning_rate": 2.0887009193132456e-05,
      "loss": 0.4454,
      "step": 1870
    },
    {
      "epoch": 0.3735810626195385,
      "grad_norm": 0.8567124009132385,
      "learning_rate": 2.0800760382938106e-05,
      "loss": 0.4532,
      "step": 1880
    },
    {
      "epoch": 0.37556819593134455,
      "grad_norm": 0.7226504683494568,
      "learning_rate": 2.071428547199742e-05,
      "loss": 0.4422,
      "step": 1890
    },
    {
      "epoch": 0.3775553292431506,
      "grad_norm": 0.8503902554512024,
      "learning_rate": 2.0627587830910207e-05,
      "loss": 0.4509,
      "step": 1900
    },
    {
      "epoch": 0.37954246255495666,
      "grad_norm": 0.785042941570282,
      "learning_rate": 2.054067083895783e-05,
      "loss": 0.4517,
      "step": 1910
    },
    {
      "epoch": 0.3815295958667627,
      "grad_norm": 0.8870737552642822,
      "learning_rate": 2.0453537883971432e-05,
      "loss": 0.4549,
      "step": 1920
    },
    {
      "epoch": 0.38351672917856877,
      "grad_norm": 0.8439989686012268,
      "learning_rate": 2.0366192362199938e-05,
      "loss": 0.4469,
      "step": 1930
    },
    {
      "epoch": 0.3855038624903748,
      "grad_norm": 0.6868243217468262,
      "learning_rate": 2.0278637678177634e-05,
      "loss": 0.4518,
      "step": 1940
    },
    {
      "epoch": 0.3874909958021809,
      "grad_norm": 0.7501059770584106,
      "learning_rate": 2.01908772445915e-05,
      "loss": 0.45,
      "step": 1950
    },
    {
      "epoch": 0.38947812911398694,
      "grad_norm": 0.8830372095108032,
      "learning_rate": 2.010291448214817e-05,
      "loss": 0.4454,
      "step": 1960
    },
    {
      "epoch": 0.391465262425793,
      "grad_norm": 0.7811934351921082,
      "learning_rate": 2.0014752819440607e-05,
      "loss": 0.4501,
      "step": 1970
    },
    {
      "epoch": 0.39345239573759905,
      "grad_norm": 0.7983005046844482,
      "learning_rate": 1.9926395692814464e-05,
      "loss": 0.457,
      "step": 1980
    },
    {
      "epoch": 0.3954395290494051,
      "grad_norm": 0.8241532444953918,
      "learning_rate": 1.9837846546234157e-05,
      "loss": 0.4515,
      "step": 1990
    },
    {
      "epoch": 0.39742666236121116,
      "grad_norm": 0.7631872892379761,
      "learning_rate": 1.9749108831148585e-05,
      "loss": 0.4465,
      "step": 2000
    },
    {
      "epoch": 0.3994137956730172,
      "grad_norm": 0.8197296261787415,
      "learning_rate": 1.966018600635665e-05,
      "loss": 0.4448,
      "step": 2010
    },
    {
      "epoch": 0.40140092898482327,
      "grad_norm": 0.7019262313842773,
      "learning_rate": 1.9571081537872418e-05,
      "loss": 0.4431,
      "step": 2020
    },
    {
      "epoch": 0.4033880622966293,
      "grad_norm": 0.7842764258384705,
      "learning_rate": 1.9481798898790016e-05,
      "loss": 0.4472,
      "step": 2030
    },
    {
      "epoch": 0.4053751956084354,
      "grad_norm": 0.7924197912216187,
      "learning_rate": 1.9392341569148254e-05,
      "loss": 0.4527,
      "step": 2040
    },
    {
      "epoch": 0.40736232892024143,
      "grad_norm": 0.7783784866333008,
      "learning_rate": 1.9302713035795022e-05,
      "loss": 0.4551,
      "step": 2050
    },
    {
      "epoch": 0.4093494622320475,
      "grad_norm": 0.7089746594429016,
      "learning_rate": 1.9212916792251326e-05,
      "loss": 0.4475,
      "step": 2060
    },
    {
      "epoch": 0.41133659554385354,
      "grad_norm": 0.8144475817680359,
      "learning_rate": 1.912295633857514e-05,
      "loss": 0.4483,
      "step": 2070
    },
    {
      "epoch": 0.4133237288556596,
      "grad_norm": 0.7984353303909302,
      "learning_rate": 1.9032835181225e-05,
      "loss": 0.4499,
      "step": 2080
    },
    {
      "epoch": 0.41531086216746566,
      "grad_norm": 0.8420728445053101,
      "learning_rate": 1.8942556832923306e-05,
      "loss": 0.4491,
      "step": 2090
    },
    {
      "epoch": 0.4172979954792717,
      "grad_norm": 0.758810818195343,
      "learning_rate": 1.8852124812519415e-05,
      "loss": 0.4467,
      "step": 2100
    },
    {
      "epoch": 0.41928512879107777,
      "grad_norm": 0.7343714833259583,
      "learning_rate": 1.8761542644852476e-05,
      "loss": 0.4483,
      "step": 2110
    },
    {
      "epoch": 0.4212722621028838,
      "grad_norm": 0.724915087223053,
      "learning_rate": 1.8670813860614043e-05,
      "loss": 0.4527,
      "step": 2120
    },
    {
      "epoch": 0.4232593954146899,
      "grad_norm": 0.7498719096183777,
      "learning_rate": 1.8579941996210476e-05,
      "loss": 0.4533,
      "step": 2130
    },
    {
      "epoch": 0.42524652872649593,
      "grad_norm": 0.6932669281959534,
      "learning_rate": 1.8488930593625044e-05,
      "loss": 0.449,
      "step": 2140
    },
    {
      "epoch": 0.427233662038302,
      "grad_norm": 0.7854167819023132,
      "learning_rate": 1.839778320027995e-05,
      "loss": 0.4403,
      "step": 2150
    },
    {
      "epoch": 0.42922079535010804,
      "grad_norm": 0.6999165415763855,
      "learning_rate": 1.8306503368897983e-05,
      "loss": 0.4456,
      "step": 2160
    },
    {
      "epoch": 0.4312079286619141,
      "grad_norm": 0.8230915665626526,
      "learning_rate": 1.8215094657364088e-05,
      "loss": 0.4526,
      "step": 2170
    },
    {
      "epoch": 0.43319506197372015,
      "grad_norm": 0.7869953513145447,
      "learning_rate": 1.8123560628586655e-05,
      "loss": 0.4522,
      "step": 2180
    },
    {
      "epoch": 0.4351821952855262,
      "grad_norm": 0.9079824090003967,
      "learning_rate": 1.803190485035868e-05,
      "loss": 0.4542,
      "step": 2190
    },
    {
      "epoch": 0.43716932859733226,
      "grad_norm": 0.7437761425971985,
      "learning_rate": 1.7940130895218676e-05,
      "loss": 0.4511,
      "step": 2200
    },
    {
      "epoch": 0.4391564619091383,
      "grad_norm": 0.7503591179847717,
      "learning_rate": 1.7848242340311426e-05,
      "loss": 0.4523,
      "step": 2210
    },
    {
      "epoch": 0.4411435952209444,
      "grad_norm": 0.8594871163368225,
      "learning_rate": 1.775624276724856e-05,
      "loss": 0.4482,
      "step": 2220
    },
    {
      "epoch": 0.44313072853275043,
      "grad_norm": 0.8280309438705444,
      "learning_rate": 1.7664135761968937e-05,
      "loss": 0.4527,
      "step": 2230
    },
    {
      "epoch": 0.4451178618445565,
      "grad_norm": 0.7477657794952393,
      "learning_rate": 1.7571924914598915e-05,
      "loss": 0.4456,
      "step": 2240
    },
    {
      "epoch": 0.44710499515636254,
      "grad_norm": 0.8169072866439819,
      "learning_rate": 1.7479613819312347e-05,
      "loss": 0.4466,
      "step": 2250
    },
    {
      "epoch": 0.4490921284681686,
      "grad_norm": 0.677374005317688,
      "learning_rate": 1.7387206074190564e-05,
      "loss": 0.4572,
      "step": 2260
    },
    {
      "epoch": 0.45107926177997465,
      "grad_norm": 0.7739725112915039,
      "learning_rate": 1.729470528108207e-05,
      "loss": 0.4427,
      "step": 2270
    },
    {
      "epoch": 0.4530663950917807,
      "grad_norm": 0.790535032749176,
      "learning_rate": 1.720211504546217e-05,
      "loss": 0.443,
      "step": 2280
    },
    {
      "epoch": 0.45505352840358676,
      "grad_norm": 0.6797199249267578,
      "learning_rate": 1.710943897629246e-05,
      "loss": 0.4473,
      "step": 2290
    },
    {
      "epoch": 0.4570406617153928,
      "grad_norm": 0.7429097294807434,
      "learning_rate": 1.7016680685880106e-05,
      "loss": 0.4523,
      "step": 2300
    },
    {
      "epoch": 0.45902779502719887,
      "grad_norm": 0.7882960438728333,
      "learning_rate": 1.6923843789737118e-05,
      "loss": 0.4491,
      "step": 2310
    },
    {
      "epoch": 0.4610149283390049,
      "grad_norm": 0.7535534501075745,
      "learning_rate": 1.6830931906439345e-05,
      "loss": 0.447,
      "step": 2320
    },
    {
      "epoch": 0.463002061650811,
      "grad_norm": 0.8358463048934937,
      "learning_rate": 1.6737948657485482e-05,
      "loss": 0.4512,
      "step": 2330
    },
    {
      "epoch": 0.46498919496261704,
      "grad_norm": 0.841930627822876,
      "learning_rate": 1.66448976671559e-05,
      "loss": 0.4517,
      "step": 2340
    },
    {
      "epoch": 0.4669763282744231,
      "grad_norm": 0.8097243309020996,
      "learning_rate": 1.6551782562371378e-05,
      "loss": 0.4431,
      "step": 2350
    },
    {
      "epoch": 0.46896346158622915,
      "grad_norm": 0.7290341854095459,
      "learning_rate": 1.6458606972551732e-05,
      "loss": 0.4525,
      "step": 2360
    },
    {
      "epoch": 0.4709505948980352,
      "grad_norm": 0.6876198053359985,
      "learning_rate": 1.6365374529474352e-05,
      "loss": 0.459,
      "step": 2370
    },
    {
      "epoch": 0.47293772820984126,
      "grad_norm": 0.807463526725769,
      "learning_rate": 1.627208886713264e-05,
      "loss": 0.4403,
      "step": 2380
    },
    {
      "epoch": 0.4749248615216473,
      "grad_norm": 0.9470881223678589,
      "learning_rate": 1.617875362159437e-05,
      "loss": 0.4491,
      "step": 2390
    },
    {
      "epoch": 0.47691199483345337,
      "grad_norm": 0.7701414823532104,
      "learning_rate": 1.6085372430859948e-05,
      "loss": 0.446,
      "step": 2400
    },
    {
      "epoch": 0.4788991281452594,
      "grad_norm": 0.7557684183120728,
      "learning_rate": 1.599194893472064e-05,
      "loss": 0.4412,
      "step": 2410
    },
    {
      "epoch": 0.4808862614570655,
      "grad_norm": 0.7397041916847229,
      "learning_rate": 1.5898486774616667e-05,
      "loss": 0.4466,
      "step": 2420
    },
    {
      "epoch": 0.48287339476887153,
      "grad_norm": 0.7878075838088989,
      "learning_rate": 1.5804989593495293e-05,
      "loss": 0.4502,
      "step": 2430
    },
    {
      "epoch": 0.4848605280806776,
      "grad_norm": 0.7665120363235474,
      "learning_rate": 1.5711461035668822e-05,
      "loss": 0.4429,
      "step": 2440
    },
    {
      "epoch": 0.48684766139248364,
      "grad_norm": 0.7302126884460449,
      "learning_rate": 1.5617904746672543e-05,
      "loss": 0.4452,
      "step": 2450
    },
    {
      "epoch": 0.4888347947042897,
      "grad_norm": 0.777780294418335,
      "learning_rate": 1.5524324373122667e-05,
      "loss": 0.4578,
      "step": 2460
    },
    {
      "epoch": 0.49082192801609575,
      "grad_norm": 0.7592859268188477,
      "learning_rate": 1.5430723562574144e-05,
      "loss": 0.4505,
      "step": 2470
    },
    {
      "epoch": 0.4928090613279018,
      "grad_norm": 0.7897220253944397,
      "learning_rate": 1.5337105963378523e-05,
      "loss": 0.4485,
      "step": 2480
    },
    {
      "epoch": 0.49479619463970786,
      "grad_norm": 0.7387880086898804,
      "learning_rate": 1.5243475224541735e-05,
      "loss": 0.4458,
      "step": 2490
    },
    {
      "epoch": 0.4967833279515139,
      "grad_norm": 0.8305100202560425,
      "learning_rate": 1.5149834995581875e-05,
      "loss": 0.4468,
      "step": 2500
    },
    {
      "epoch": 0.49877046126332003,
      "grad_norm": 0.8320267200469971,
      "learning_rate": 1.5056188926386923e-05,
      "loss": 0.4455,
      "step": 2510
    },
    {
      "epoch": 0.500757594575126,
      "grad_norm": 0.7625274062156677,
      "learning_rate": 1.4962540667072513e-05,
      "loss": 0.4466,
      "step": 2520
    },
    {
      "epoch": 0.5027447278869321,
      "grad_norm": 0.8282254934310913,
      "learning_rate": 1.4868893867839636e-05,
      "loss": 0.448,
      "step": 2530
    },
    {
      "epoch": 0.5047318611987381,
      "grad_norm": 0.7194404006004333,
      "learning_rate": 1.4775252178832382e-05,
      "loss": 0.4408,
      "step": 2540
    },
    {
      "epoch": 0.5067189945105443,
      "grad_norm": 0.7871652841567993,
      "learning_rate": 1.4681619249995647e-05,
      "loss": 0.4402,
      "step": 2550
    },
    {
      "epoch": 0.5087061278223503,
      "grad_norm": 0.8208021521568298,
      "learning_rate": 1.4587998730932885e-05,
      "loss": 0.4467,
      "step": 2560
    },
    {
      "epoch": 0.5106932611341564,
      "grad_norm": 0.7824015617370605,
      "learning_rate": 1.4494394270763827e-05,
      "loss": 0.4555,
      "step": 2570
    },
    {
      "epoch": 0.5126803944459624,
      "grad_norm": 0.7161893248558044,
      "learning_rate": 1.4400809517982286e-05,
      "loss": 0.4549,
      "step": 2580
    },
    {
      "epoch": 0.5146675277577685,
      "grad_norm": 0.8560624718666077,
      "learning_rate": 1.430724812031391e-05,
      "loss": 0.4455,
      "step": 2590
    },
    {
      "epoch": 0.5166546610695745,
      "grad_norm": 0.8465664982795715,
      "learning_rate": 1.4213713724574016e-05,
      "loss": 0.4465,
      "step": 2600
    },
    {
      "epoch": 0.5186417943813806,
      "grad_norm": 0.8016757965087891,
      "learning_rate": 1.4120209976525454e-05,
      "loss": 0.4448,
      "step": 2610
    },
    {
      "epoch": 0.5206289276931866,
      "grad_norm": 0.7907713651657104,
      "learning_rate": 1.4026740520736493e-05,
      "loss": 0.4469,
      "step": 2620
    },
    {
      "epoch": 0.5226160610049927,
      "grad_norm": 0.7759795188903809,
      "learning_rate": 1.3933309000438767e-05,
      "loss": 0.4449,
      "step": 2630
    },
    {
      "epoch": 0.5246031943167987,
      "grad_norm": 0.7276362776756287,
      "learning_rate": 1.3839919057385259e-05,
      "loss": 0.4441,
      "step": 2640
    },
    {
      "epoch": 0.5265903276286048,
      "grad_norm": 0.8391414880752563,
      "learning_rate": 1.3746574331708374e-05,
      "loss": 0.4504,
      "step": 2650
    },
    {
      "epoch": 0.5285774609404108,
      "grad_norm": 0.8051692247390747,
      "learning_rate": 1.3653278461778045e-05,
      "loss": 0.4494,
      "step": 2660
    },
    {
      "epoch": 0.5305645942522169,
      "grad_norm": 0.7826213240623474,
      "learning_rate": 1.3560035084059905e-05,
      "loss": 0.4413,
      "step": 2670
    },
    {
      "epoch": 0.5325517275640229,
      "grad_norm": 0.7661068439483643,
      "learning_rate": 1.3466847832973569e-05,
      "loss": 0.4517,
      "step": 2680
    },
    {
      "epoch": 0.534538860875829,
      "grad_norm": 0.7276708483695984,
      "learning_rate": 1.3373720340750955e-05,
      "loss": 0.453,
      "step": 2690
    },
    {
      "epoch": 0.536525994187635,
      "grad_norm": 0.8548576235771179,
      "learning_rate": 1.328065623729472e-05,
      "loss": 0.4459,
      "step": 2700
    },
    {
      "epoch": 0.5385131274994411,
      "grad_norm": 0.7641034722328186,
      "learning_rate": 1.3187659150036754e-05,
      "loss": 0.446,
      "step": 2710
    },
    {
      "epoch": 0.5405002608112471,
      "grad_norm": 0.7968651056289673,
      "learning_rate": 1.3094732703796819e-05,
      "loss": 0.4455,
      "step": 2720
    },
    {
      "epoch": 0.5424873941230532,
      "grad_norm": 0.7762000560760498,
      "learning_rate": 1.3001880520641243e-05,
      "loss": 0.4448,
      "step": 2730
    },
    {
      "epoch": 0.5444745274348592,
      "grad_norm": 0.8031491041183472,
      "learning_rate": 1.2909106219741734e-05,
      "loss": 0.4513,
      "step": 2740
    },
    {
      "epoch": 0.5464616607466654,
      "grad_norm": 0.7698557376861572,
      "learning_rate": 1.2816413417234336e-05,
      "loss": 0.4448,
      "step": 2750
    },
    {
      "epoch": 0.5484487940584714,
      "grad_norm": 0.7751819491386414,
      "learning_rate": 1.272380572607846e-05,
      "loss": 0.452,
      "step": 2760
    },
    {
      "epoch": 0.5504359273702775,
      "grad_norm": 0.8459222912788391,
      "learning_rate": 1.2631286755916075e-05,
      "loss": 0.4485,
      "step": 2770
    },
    {
      "epoch": 0.5524230606820835,
      "grad_norm": 0.8367515802383423,
      "learning_rate": 1.2538860112930986e-05,
      "loss": 0.4455,
      "step": 2780
    },
    {
      "epoch": 0.5544101939938896,
      "grad_norm": 0.7809949517250061,
      "learning_rate": 1.2446529399708301e-05,
      "loss": 0.4447,
      "step": 2790
    },
    {
      "epoch": 0.5563973273056956,
      "grad_norm": 0.7862070202827454,
      "learning_rate": 1.2354298215094004e-05,
      "loss": 0.4489,
      "step": 2800
    },
    {
      "epoch": 0.5583844606175017,
      "grad_norm": 0.7013892531394958,
      "learning_rate": 1.226217015405466e-05,
      "loss": 0.4396,
      "step": 2810
    },
    {
      "epoch": 0.5603715939293077,
      "grad_norm": 0.7742162346839905,
      "learning_rate": 1.2170148807537317e-05,
      "loss": 0.4495,
      "step": 2820
    },
    {
      "epoch": 0.5623587272411138,
      "grad_norm": 0.7877035737037659,
      "learning_rate": 1.2078237762329527e-05,
      "loss": 0.4482,
      "step": 2830
    },
    {
      "epoch": 0.5643458605529198,
      "grad_norm": 0.8228877782821655,
      "learning_rate": 1.1986440600919544e-05,
      "loss": 0.445,
      "step": 2840
    },
    {
      "epoch": 0.5663329938647259,
      "grad_norm": 0.8021484613418579,
      "learning_rate": 1.1894760901356676e-05,
      "loss": 0.4531,
      "step": 2850
    },
    {
      "epoch": 0.5683201271765319,
      "grad_norm": 0.7744923233985901,
      "learning_rate": 1.1803202237111837e-05,
      "loss": 0.4397,
      "step": 2860
    },
    {
      "epoch": 0.570307260488338,
      "grad_norm": 0.7884088158607483,
      "learning_rate": 1.1711768176938253e-05,
      "loss": 0.441,
      "step": 2870
    },
    {
      "epoch": 0.572294393800144,
      "grad_norm": 0.8151083588600159,
      "learning_rate": 1.1620462284732356e-05,
      "loss": 0.4549,
      "step": 2880
    },
    {
      "epoch": 0.5742815271119501,
      "grad_norm": 0.8962993621826172,
      "learning_rate": 1.1529288119394878e-05,
      "loss": 0.4483,
      "step": 2890
    },
    {
      "epoch": 0.5762686604237561,
      "grad_norm": 0.7281681895256042,
      "learning_rate": 1.143824923469213e-05,
      "loss": 0.4586,
      "step": 2900
    },
    {
      "epoch": 0.5782557937355622,
      "grad_norm": 0.8384059071540833,
      "learning_rate": 1.1347349179117491e-05,
      "loss": 0.4483,
      "step": 2910
    },
    {
      "epoch": 0.5802429270473682,
      "grad_norm": 0.6807173490524292,
      "learning_rate": 1.1256591495753075e-05,
      "loss": 0.4382,
      "step": 2920
    },
    {
      "epoch": 0.5822300603591744,
      "grad_norm": 0.7415173649787903,
      "learning_rate": 1.1165979722131655e-05,
      "loss": 0.4428,
      "step": 2930
    },
    {
      "epoch": 0.5842171936709804,
      "grad_norm": 0.8389567732810974,
      "learning_rate": 1.1075517390098766e-05,
      "loss": 0.4451,
      "step": 2940
    },
    {
      "epoch": 0.5862043269827865,
      "grad_norm": 0.7635670900344849,
      "learning_rate": 1.0985208025675035e-05,
      "loss": 0.4383,
      "step": 2950
    },
    {
      "epoch": 0.5881914602945925,
      "grad_norm": 0.8206814527511597,
      "learning_rate": 1.0895055148918758e-05,
      "loss": 0.4524,
      "step": 2960
    },
    {
      "epoch": 0.5901785936063986,
      "grad_norm": 0.8443226218223572,
      "learning_rate": 1.0805062273788686e-05,
      "loss": 0.4531,
      "step": 2970
    },
    {
      "epoch": 0.5921657269182046,
      "grad_norm": 0.8645696043968201,
      "learning_rate": 1.071523290800707e-05,
      "loss": 0.4419,
      "step": 2980
    },
    {
      "epoch": 0.5941528602300107,
      "grad_norm": 0.7823992371559143,
      "learning_rate": 1.0625570552922913e-05,
      "loss": 0.4451,
      "step": 2990
    },
    {
      "epoch": 0.5961399935418167,
      "grad_norm": 0.7402434349060059,
      "learning_rate": 1.0536078703375533e-05,
      "loss": 0.4498,
      "step": 3000
    },
    {
      "epoch": 0.5981271268536228,
      "grad_norm": 0.800948977470398,
      "learning_rate": 1.0446760847558313e-05,
      "loss": 0.4386,
      "step": 3010
    },
    {
      "epoch": 0.6001142601654289,
      "grad_norm": 0.8192581534385681,
      "learning_rate": 1.0357620466882728e-05,
      "loss": 0.4484,
      "step": 3020
    },
    {
      "epoch": 0.6021013934772349,
      "grad_norm": 0.8546594977378845,
      "learning_rate": 1.0268661035842701e-05,
      "loss": 0.4372,
      "step": 3030
    },
    {
      "epoch": 0.604088526789041,
      "grad_norm": 0.7593885660171509,
      "learning_rate": 1.0179886021879117e-05,
      "loss": 0.4454,
      "step": 3040
    },
    {
      "epoch": 0.606075660100847,
      "grad_norm": 0.7293996810913086,
      "learning_rate": 1.0091298885244705e-05,
      "loss": 0.4403,
      "step": 3050
    },
    {
      "epoch": 0.6080627934126531,
      "grad_norm": 0.726388692855835,
      "learning_rate": 1.0002903078869137e-05,
      "loss": 0.446,
      "step": 3060
    },
    {
      "epoch": 0.6100499267244591,
      "grad_norm": 0.7083622813224792,
      "learning_rate": 9.914702048224475e-06,
      "loss": 0.4414,
      "step": 3070
    },
    {
      "epoch": 0.6120370600362652,
      "grad_norm": 0.779699981212616,
      "learning_rate": 9.826699231190852e-06,
      "loss": 0.4476,
      "step": 3080
    },
    {
      "epoch": 0.6140241933480712,
      "grad_norm": 0.7660971283912659,
      "learning_rate": 9.73889805792246e-06,
      "loss": 0.4443,
      "step": 3090
    },
    {
      "epoch": 0.6160113266598773,
      "grad_norm": 0.7091546654701233,
      "learning_rate": 9.651301950713886e-06,
      "loss": 0.4427,
      "step": 3100
    },
    {
      "epoch": 0.6179984599716833,
      "grad_norm": 0.7612859606742859,
      "learning_rate": 9.563914323866683e-06,
      "loss": 0.4437,
      "step": 3110
    },
    {
      "epoch": 0.6199855932834895,
      "grad_norm": 0.809298574924469,
      "learning_rate": 9.47673858355632e-06,
      "loss": 0.446,
      "step": 3120
    },
    {
      "epoch": 0.6219727265952955,
      "grad_norm": 0.6963706612586975,
      "learning_rate": 9.389778127699369e-06,
      "loss": 0.4425,
      "step": 3130
    },
    {
      "epoch": 0.6239598599071016,
      "grad_norm": 0.7808335423469543,
      "learning_rate": 9.303036345821127e-06,
      "loss": 0.4524,
      "step": 3140
    },
    {
      "epoch": 0.6259469932189076,
      "grad_norm": 0.8052681684494019,
      "learning_rate": 9.21651661892345e-06,
      "loss": 0.4398,
      "step": 3150
    },
    {
      "epoch": 0.6279341265307137,
      "grad_norm": 0.8464260101318359,
      "learning_rate": 9.130222319352974e-06,
      "loss": 0.4447,
      "step": 3160
    },
    {
      "epoch": 0.6299212598425197,
      "grad_norm": 0.8108941912651062,
      "learning_rate": 9.044156810669693e-06,
      "loss": 0.4462,
      "step": 3170
    },
    {
      "epoch": 0.6319083931543258,
      "grad_norm": 0.7537918090820312,
      "learning_rate": 8.958323447515848e-06,
      "loss": 0.4351,
      "step": 3180
    },
    {
      "epoch": 0.6338955264661318,
      "grad_norm": 0.7738850116729736,
      "learning_rate": 8.872725575485137e-06,
      "loss": 0.4382,
      "step": 3190
    },
    {
      "epoch": 0.6358826597779379,
      "grad_norm": 0.8285481929779053,
      "learning_rate": 8.78736653099236e-06,
      "loss": 0.4448,
      "step": 3200
    },
    {
      "epoch": 0.6378697930897439,
      "grad_norm": 0.8191623687744141,
      "learning_rate": 8.702249641143341e-06,
      "loss": 0.4466,
      "step": 3210
    },
    {
      "epoch": 0.63985692640155,
      "grad_norm": 0.74721759557724,
      "learning_rate": 8.61737822360526e-06,
      "loss": 0.4434,
      "step": 3220
    },
    {
      "epoch": 0.641844059713356,
      "grad_norm": 0.7885379791259766,
      "learning_rate": 8.532755586477326e-06,
      "loss": 0.4485,
      "step": 3230
    },
    {
      "epoch": 0.6438311930251621,
      "grad_norm": 0.8131423592567444,
      "learning_rate": 8.448385028161842e-06,
      "loss": 0.4428,
      "step": 3240
    },
    {
      "epoch": 0.6458183263369681,
      "grad_norm": 0.7771499752998352,
      "learning_rate": 8.364269837235648e-06,
      "loss": 0.4378,
      "step": 3250
    },
    {
      "epoch": 0.6478054596487742,
      "grad_norm": 0.818691074848175,
      "learning_rate": 8.28041329232191e-06,
      "loss": 0.4495,
      "step": 3260
    },
    {
      "epoch": 0.6497925929605802,
      "grad_norm": 0.7409109473228455,
      "learning_rate": 8.196818661962367e-06,
      "loss": 0.442,
      "step": 3270
    },
    {
      "epoch": 0.6517797262723863,
      "grad_norm": 0.766283392906189,
      "learning_rate": 8.113489204489904e-06,
      "loss": 0.4438,
      "step": 3280
    },
    {
      "epoch": 0.6537668595841923,
      "grad_norm": 0.8754457831382751,
      "learning_rate": 8.030428167901556e-06,
      "loss": 0.4518,
      "step": 3290
    },
    {
      "epoch": 0.6557539928959984,
      "grad_norm": 0.8813508749008179,
      "learning_rate": 7.947638789731908e-06,
      "loss": 0.4533,
      "step": 3300
    },
    {
      "epoch": 0.6577411262078044,
      "grad_norm": 0.7640528678894043,
      "learning_rate": 7.865124296926908e-06,
      "loss": 0.4487,
      "step": 3310
    },
    {
      "epoch": 0.6597282595196106,
      "grad_norm": 0.7498531341552734,
      "learning_rate": 7.782887905718079e-06,
      "loss": 0.4435,
      "step": 3320
    },
    {
      "epoch": 0.6617153928314166,
      "grad_norm": 0.8000423312187195,
      "learning_rate": 7.700932821497157e-06,
      "loss": 0.4435,
      "step": 3330
    },
    {
      "epoch": 0.6637025261432227,
      "grad_norm": 0.8384105563163757,
      "learning_rate": 7.619262238691169e-06,
      "loss": 0.4412,
      "step": 3340
    },
    {
      "epoch": 0.6656896594550287,
      "grad_norm": 0.8713394999504089,
      "learning_rate": 7.537879340637901e-06,
      "loss": 0.4394,
      "step": 3350
    },
    {
      "epoch": 0.6676767927668348,
      "grad_norm": 0.8085681200027466,
      "learning_rate": 7.456787299461828e-06,
      "loss": 0.4481,
      "step": 3360
    },
    {
      "epoch": 0.6696639260786408,
      "grad_norm": 0.7761397957801819,
      "learning_rate": 7.3759892759504704e-06,
      "loss": 0.4429,
      "step": 3370
    },
    {
      "epoch": 0.6716510593904469,
      "grad_norm": 0.7563180327415466,
      "learning_rate": 7.2954884194311985e-06,
      "loss": 0.4448,
      "step": 3380
    },
    {
      "epoch": 0.6736381927022529,
      "grad_norm": 0.8492968082427979,
      "learning_rate": 7.215287867648472e-06,
      "loss": 0.4499,
      "step": 3390
    },
    {
      "epoch": 0.675625326014059,
      "grad_norm": 0.8029958009719849,
      "learning_rate": 7.135390746641527e-06,
      "loss": 0.4381,
      "step": 3400
    },
    {
      "epoch": 0.677612459325865,
      "grad_norm": 0.8523545265197754,
      "learning_rate": 7.055800170622557e-06,
      "loss": 0.4477,
      "step": 3410
    },
    {
      "epoch": 0.6795995926376711,
      "grad_norm": 0.7670935988426208,
      "learning_rate": 6.97651924185531e-06,
      "loss": 0.4435,
      "step": 3420
    },
    {
      "epoch": 0.6815867259494771,
      "grad_norm": 0.7864395380020142,
      "learning_rate": 6.897551050534172e-06,
      "loss": 0.4415,
      "step": 3430
    },
    {
      "epoch": 0.6835738592612832,
      "grad_norm": 0.826335608959198,
      "learning_rate": 6.818898674663716e-06,
      "loss": 0.4426,
      "step": 3440
    },
    {
      "epoch": 0.6855609925730892,
      "grad_norm": 0.817119836807251,
      "learning_rate": 6.7405651799387305e-06,
      "loss": 0.4352,
      "step": 3450
    },
    {
      "epoch": 0.6875481258848953,
      "grad_norm": 0.7957607507705688,
      "learning_rate": 6.662553619624736e-06,
      "loss": 0.4392,
      "step": 3460
    },
    {
      "epoch": 0.6895352591967013,
      "grad_norm": 0.7614489793777466,
      "learning_rate": 6.584867034438944e-06,
      "loss": 0.4475,
      "step": 3470
    },
    {
      "epoch": 0.6915223925085074,
      "grad_norm": 0.8026480078697205,
      "learning_rate": 6.507508452431779e-06,
      "loss": 0.4487,
      "step": 3480
    },
    {
      "epoch": 0.6935095258203134,
      "grad_norm": 0.7840617895126343,
      "learning_rate": 6.430480888868817e-06,
      "loss": 0.4392,
      "step": 3490
    },
    {
      "epoch": 0.6954966591321196,
      "grad_norm": 0.8650174736976624,
      "learning_rate": 6.353787346113287e-06,
      "loss": 0.4291,
      "step": 3500
    },
    {
      "epoch": 0.6974837924439256,
      "grad_norm": 0.7454703450202942,
      "learning_rate": 6.277430813508997e-06,
      "loss": 0.4424,
      "step": 3510
    },
    {
      "epoch": 0.6994709257557317,
      "grad_norm": 0.8037274479866028,
      "learning_rate": 6.2014142672638895e-06,
      "loss": 0.4456,
      "step": 3520
    },
    {
      "epoch": 0.7014580590675377,
      "grad_norm": 0.7770602703094482,
      "learning_rate": 6.125740670333973e-06,
      "loss": 0.4481,
      "step": 3530
    },
    {
      "epoch": 0.7034451923793438,
      "grad_norm": 0.7782578468322754,
      "learning_rate": 6.050412972307847e-06,
      "loss": 0.4505,
      "step": 3540
    },
    {
      "epoch": 0.7054323256911498,
      "grad_norm": 0.8028565645217896,
      "learning_rate": 5.9754341092917516e-06,
      "loss": 0.4499,
      "step": 3550
    },
    {
      "epoch": 0.7074194590029559,
      "grad_norm": 0.8082902431488037,
      "learning_rate": 5.900807003795112e-06,
      "loss": 0.4453,
      "step": 3560
    },
    {
      "epoch": 0.7094065923147619,
      "grad_norm": 0.8421599864959717,
      "learning_rate": 5.826534564616634e-06,
      "loss": 0.4429,
      "step": 3570
    },
    {
      "epoch": 0.711393725626568,
      "grad_norm": 0.802895188331604,
      "learning_rate": 5.752619686730892e-06,
      "loss": 0.4431,
      "step": 3580
    },
    {
      "epoch": 0.713380858938374,
      "grad_norm": 0.9135997891426086,
      "learning_rate": 5.679065251175552e-06,
      "loss": 0.4452,
      "step": 3590
    },
    {
      "epoch": 0.7153679922501801,
      "grad_norm": 0.8532732129096985,
      "learning_rate": 5.605874124939021e-06,
      "loss": 0.442,
      "step": 3600
    },
    {
      "epoch": 0.7173551255619861,
      "grad_norm": 0.7450631856918335,
      "learning_rate": 5.533049160848709e-06,
      "loss": 0.4433,
      "step": 3610
    },
    {
      "epoch": 0.7193422588737922,
      "grad_norm": 0.8432106375694275,
      "learning_rate": 5.46059319745985e-06,
      "loss": 0.4529,
      "step": 3620
    },
    {
      "epoch": 0.7213293921855982,
      "grad_norm": 0.8421300053596497,
      "learning_rate": 5.388509058944851e-06,
      "loss": 0.4395,
      "step": 3630
    },
    {
      "epoch": 0.7233165254974043,
      "grad_norm": 0.8038370013237,
      "learning_rate": 5.316799554983214e-06,
      "loss": 0.4369,
      "step": 3640
    },
    {
      "epoch": 0.7253036588092103,
      "grad_norm": 0.8200697302818298,
      "learning_rate": 5.2454674806520025e-06,
      "loss": 0.4428,
      "step": 3650
    },
    {
      "epoch": 0.7272907921210164,
      "grad_norm": 0.8071765899658203,
      "learning_rate": 5.174515616316926e-06,
      "loss": 0.4442,
      "step": 3660
    },
    {
      "epoch": 0.7292779254328224,
      "grad_norm": 0.80715012550354,
      "learning_rate": 5.103946727523964e-06,
      "loss": 0.4437,
      "step": 3670
    },
    {
      "epoch": 0.7312650587446285,
      "grad_norm": 0.8524712920188904,
      "learning_rate": 5.033763564891536e-06,
      "loss": 0.4509,
      "step": 3680
    },
    {
      "epoch": 0.7332521920564345,
      "grad_norm": 0.8193678855895996,
      "learning_rate": 4.963968864003325e-06,
      "loss": 0.436,
      "step": 3690
    },
    {
      "epoch": 0.7352393253682407,
      "grad_norm": 0.825983464717865,
      "learning_rate": 4.894565345301642e-06,
      "loss": 0.4426,
      "step": 3700
    },
    {
      "epoch": 0.7372264586800467,
      "grad_norm": 0.7969499230384827,
      "learning_rate": 4.825555713981385e-06,
      "loss": 0.4425,
      "step": 3710
    },
    {
      "epoch": 0.7392135919918528,
      "grad_norm": 0.8560088276863098,
      "learning_rate": 4.756942659884589e-06,
      "loss": 0.4515,
      "step": 3720
    },
    {
      "epoch": 0.7412007253036588,
      "grad_norm": 0.7600152492523193,
      "learning_rate": 4.6887288573955894e-06,
      "loss": 0.4407,
      "step": 3730
    },
    {
      "epoch": 0.7431878586154649,
      "grad_norm": 0.8174750208854675,
      "learning_rate": 4.620916965336809e-06,
      "loss": 0.4523,
      "step": 3740
    },
    {
      "epoch": 0.7451749919272709,
      "grad_norm": 0.7266955971717834,
      "learning_rate": 4.553509626865055e-06,
      "loss": 0.4438,
      "step": 3750
    },
    {
      "epoch": 0.747162125239077,
      "grad_norm": 0.8405449390411377,
      "learning_rate": 4.486509469368563e-06,
      "loss": 0.4349,
      "step": 3760
    },
    {
      "epoch": 0.749149258550883,
      "grad_norm": 0.8286876082420349,
      "learning_rate": 4.419919104364547e-06,
      "loss": 0.4361,
      "step": 3770
    },
    {
      "epoch": 0.7511363918626891,
      "grad_norm": 0.859610378742218,
      "learning_rate": 4.353741127397432e-06,
      "loss": 0.4377,
      "step": 3780
    },
    {
      "epoch": 0.7531235251744951,
      "grad_norm": 0.7775627970695496,
      "learning_rate": 4.287978117937654e-06,
      "loss": 0.4468,
      "step": 3790
    },
    {
      "epoch": 0.7551106584863012,
      "grad_norm": 0.7420094609260559,
      "learning_rate": 4.222632639281155e-06,
      "loss": 0.4367,
      "step": 3800
    },
    {
      "epoch": 0.7570977917981072,
      "grad_norm": 0.8359178900718689,
      "learning_rate": 4.157707238449451e-06,
      "loss": 0.4497,
      "step": 3810
    },
    {
      "epoch": 0.7590849251099133,
      "grad_norm": 0.8115233182907104,
      "learning_rate": 4.093204446090354e-06,
      "loss": 0.4406,
      "step": 3820
    },
    {
      "epoch": 0.7610720584217193,
      "grad_norm": 0.7380189895629883,
      "learning_rate": 4.02912677637934e-06,
      "loss": 0.4408,
      "step": 3830
    },
    {
      "epoch": 0.7630591917335254,
      "grad_norm": 0.7655649185180664,
      "learning_rate": 3.965476726921549e-06,
      "loss": 0.4451,
      "step": 3840
    },
    {
      "epoch": 0.7650463250453314,
      "grad_norm": 0.8307855725288391,
      "learning_rate": 3.9022567786544376e-06,
      "loss": 0.438,
      "step": 3850
    },
    {
      "epoch": 0.7670334583571375,
      "grad_norm": 0.7978384494781494,
      "learning_rate": 3.839469395751062e-06,
      "loss": 0.4434,
      "step": 3860
    },
    {
      "epoch": 0.7690205916689435,
      "grad_norm": 0.8367170691490173,
      "learning_rate": 3.7771170255240517e-06,
      "loss": 0.4453,
      "step": 3870
    },
    {
      "epoch": 0.7710077249807497,
      "grad_norm": 0.8245114088058472,
      "learning_rate": 3.7152020983302097e-06,
      "loss": 0.4335,
      "step": 3880
    },
    {
      "epoch": 0.7729948582925557,
      "grad_norm": 0.8620235323905945,
      "learning_rate": 3.6537270274757777e-06,
      "loss": 0.441,
      "step": 3890
    },
    {
      "epoch": 0.7749819916043618,
      "grad_norm": 1.0114320516586304,
      "learning_rate": 3.592694209122378e-06,
      "loss": 0.4475,
      "step": 3900
    },
    {
      "epoch": 0.7769691249161678,
      "grad_norm": 0.7783913016319275,
      "learning_rate": 3.5321060221936153e-06,
      "loss": 0.4447,
      "step": 3910
    },
    {
      "epoch": 0.7789562582279739,
      "grad_norm": 0.896019697189331,
      "learning_rate": 3.4719648282823375e-06,
      "loss": 0.4428,
      "step": 3920
    },
    {
      "epoch": 0.7809433915397799,
      "grad_norm": 0.7894255518913269,
      "learning_rate": 3.4122729715586125e-06,
      "loss": 0.4419,
      "step": 3930
    },
    {
      "epoch": 0.782930524851586,
      "grad_norm": 0.9319736361503601,
      "learning_rate": 3.3530327786783353e-06,
      "loss": 0.4526,
      "step": 3940
    },
    {
      "epoch": 0.784917658163392,
      "grad_norm": 0.888184666633606,
      "learning_rate": 3.294246558692553e-06,
      "loss": 0.4487,
      "step": 3950
    },
    {
      "epoch": 0.7869047914751981,
      "grad_norm": 0.868547260761261,
      "learning_rate": 3.2359166029574565e-06,
      "loss": 0.4411,
      "step": 3960
    },
    {
      "epoch": 0.7888919247870041,
      "grad_norm": 0.8741538524627686,
      "learning_rate": 3.1780451850450687e-06,
      "loss": 0.4484,
      "step": 3970
    },
    {
      "epoch": 0.7908790580988102,
      "grad_norm": 0.8805731534957886,
      "learning_rate": 3.1206345606546335e-06,
      "loss": 0.4433,
      "step": 3980
    },
    {
      "epoch": 0.7928661914106162,
      "grad_norm": 0.7595651745796204,
      "learning_rate": 3.063686967524675e-06,
      "loss": 0.4411,
      "step": 3990
    },
    {
      "epoch": 0.7948533247224223,
      "grad_norm": 0.776249885559082,
      "learning_rate": 3.0072046253457965e-06,
      "loss": 0.4372,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 5032,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.302357082692649e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
